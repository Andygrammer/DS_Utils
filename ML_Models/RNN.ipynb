{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN) Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source (all credits to):** *Easy TensorFlow*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../resources/r_nn_fixed.png\" style=\"width:80%\">\n",
    "\n",
    "2 examples:\n",
    "\n",
    "1. RNN with **fixed** sequence length.\n",
    "2. RNN with **variable** sequence length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "# use TensorFlow v.1\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Training-set size:\t\t100\n",
      "- Test-set size:\t5\n"
     ]
    }
   ],
   "source": [
    "x_train = np.random.randint(0, 10, size=(100, 4, 1))\n",
    "y_train = np.sum(x_train, axis=1)\n",
    "\n",
    "x_test = np.random.randint(0, 10, size=(5, 4, 1))\n",
    "y_test = np.sum(x_test, axis=1)\n",
    "\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set size:\\t\\t{}\".format(len(y_train)))\n",
    "print(\"- Test-set size:\\t{}\".format(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Look at Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TOP 5:\n",
      "X_train: [[2 2 0 9]]; y_train: [13]\n",
      "X_train: [[1 0 0 6]]; y_train: [7]\n",
      "X_train: [[4 5 8 7]]; y_train: [24]\n",
      "X_train: [[6 7 4 1]]; y_train: [18]\n",
      "X_train: [[7 2 8 1]]; y_train: [18]\n",
      "---\n",
      "Test TOP 5:\n",
      "X_test: [[0 9 6 2]]; y_test: [17]\n",
      "X_test: [[7 2 7 1]]; y_test: [17]\n",
      "X_test: [[3 7 5 8]]; y_test: [23]\n",
      "X_test: [[1 3 0 7]]; y_test: [11]\n",
      "X_test: [[4 5 1 2]]; y_test: [12]\n"
     ]
    }
   ],
   "source": [
    "print('Train TOP 5:')\n",
    "for idx in range(0, 5):\n",
    "    print('X_train: {}; y_train: {}'.format(x_train[idx].T, y_train[idx]))\n",
    "    \n",
    "print('---')\n",
    "print('Test TOP 5:')\n",
    "for idx in range(0, 5):\n",
    "    print('X_test: {}; y_test: {}'.format(x_test[idx].T, y_test[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Example 1 - RNN with Fixed Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py:1761: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, MSE=253.41464233398438\n",
      "Step 1000, MSE=170.3509979248047\n",
      "Step 2000, MSE=31.76686668395996\n",
      "Step 3000, MSE=23.367515563964844\n",
      "Step 4000, MSE=1.276302456855774\n",
      "Step 5000, MSE=0.9381116032600403\n",
      "Step 6000, MSE=0.09752456843852997\n",
      "Step 7000, MSE=0.5929590463638306\n",
      "Step 8000, MSE=0.10749709606170654\n",
      "Step 9000, MSE=0.22929659485816956\n",
      "---\n",
      "Test:\n",
      "When the ground truth output is [17], the model thinks it is [17.367632]\n",
      "When the ground truth output is [17], the model thinks it is [17.386267]\n",
      "When the ground truth output is [23], the model thinks it is [22.94692]\n",
      "When the ground truth output is [11], the model thinks it is [11.077478]\n",
      "When the ground truth output is [12], the model thinks it is [12.041155]\n"
     ]
    }
   ],
   "source": [
    "# data dimensions\n",
    "input_dim = 1\n",
    "seq_max_len = len(x_train[0]) # sequence maximum length\n",
    "out_dim = len(y_train[0])\n",
    "\n",
    "# clear context\n",
    "K.clear_session()\n",
    "\n",
    "# hyper-parameters\n",
    "EPOCHS = 10000\n",
    "BATCH_SIZE = 10\n",
    "DISPLAY_FREQ = 1000      # frequency of displaying the training results\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_HIDDEN_UNITS = 10    # number of hidden units\n",
    "\n",
    "# model input data\n",
    "x = tf.placeholder(tf.float32, shape=[None, seq_max_len, input_dim], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, input_dim], name='Y')\n",
    " \n",
    "# model parameters: W (weights) and b (bias)\n",
    "W = weight_variable(shape=[NUM_HIDDEN_UNITS, out_dim])\n",
    "b = bias_variable(shape=[out_dim])\n",
    "\n",
    "# calculate logits (forward output)\n",
    "pred_out = RNN(x, W, b, NUM_HIDDEN_UNITS)\n",
    "\n",
    "######## prepare model ########\n",
    "\n",
    "# loss function = mean-squared error\n",
    "cost = tf.reduce_mean(tf.square(pred_out - y))\n",
    "\n",
    "# optimizer = Adam\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE, name='Adam-op').minimize(cost)\n",
    " \n",
    "# initialize tf variables\n",
    "init = tf.global_variables_initializer()\n",
    " \n",
    "######## train model ########\n",
    "\n",
    "# create a tf session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# initialize session variables\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    x_batch, y_batch = next_batch(x_train, y_train, BATCH_SIZE)\n",
    "    _, mse = sess.run([optimizer, cost], feed_dict={x: x_batch, y: y_batch})\n",
    "    \n",
    "    if i % DISPLAY_FREQ == 0:\n",
    "        print('Step {}, MSE={}'.format(i, mse))\n",
    "        \n",
    "# validation\n",
    "y_pred = sess.run(pred_out, feed_dict={x: x_test})\n",
    "    \n",
    "######## test model ########\n",
    "\n",
    "y_pred = sess.run(pred_out, feed_dict={x: x_test})\n",
    "\n",
    "print('---')\n",
    "print('Test:')\n",
    "\n",
    "for i, x in enumerate(y_test):\n",
    "    print(\"When the ground truth output is {}, the model thinks it is {}\"\n",
    "          .format(y_test[i], y_pred[i]))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Example 2 - RNN with Variable Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Training---------\n",
      "Step 0     , MSE=377.3354\n",
      "Step 1000  , MSE=108.3779\n",
      "Step 2000  , MSE=22.5980\n",
      "Step 3000  , MSE=33.2016\n",
      "Step 4000  , MSE=10.7104\n",
      "Step 5000  , MSE=9.9249\n",
      "Step 6000  , MSE=22.7768\n",
      "Step 7000  , MSE=12.8159\n",
      "Step 8000  , MSE=12.4850\n",
      "Step 9000  , MSE=24.1962\n",
      "\n",
      "--------Test Results-------\n",
      "When the ground truth output is [17], the model thinks it is [19.231289]\n",
      "When the ground truth output is [17], the model thinks it is [18.118595]\n",
      "When the ground truth output is [23], the model thinks it is [19.52219]\n",
      "When the ground truth output is [11], the model thinks it is [13.784103]\n",
      "When the ground truth output is [12], the model thinks it is [11.877954]\n"
     ]
    }
   ],
   "source": [
    "# data dimensions\n",
    "input_dim = 1\n",
    "seq_max_len = len(x_train[0]) # sequence maximum length\n",
    "out_dim = len(y_train[0])\n",
    "seq_len_train = np.random.randint(1, seq_max_len+1, len(y_train))\n",
    "seq_len_test = np.random.randint(1, seq_max_len+1, len(y_test))\n",
    "\n",
    "# clear context\n",
    "K.clear_session()\n",
    "\n",
    "# hyper-parameters\n",
    "EPOCHS = 10000\n",
    "BATCH_SIZE = 10\n",
    "DISPLAY_FREQ = 1000      # frequency of displaying the training results\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_HIDDEN_UNITS = 10    # number of hidden units\n",
    "\n",
    "# model input data\n",
    "x = tf.placeholder(tf.float32, shape=[None, seq_max_len, input_dim], name='X')\n",
    "seqLen = tf.placeholder(tf.int32, [None])\n",
    "y = tf.placeholder(tf.float32, shape=[None, input_dim], name='Y')\n",
    " \n",
    "# model parameters: W (weights) and b (bias)\n",
    "W = weight_variable(shape=[NUM_HIDDEN_UNITS, out_dim])\n",
    "b = bias_variable(shape=[out_dim])\n",
    "\n",
    "# calculate logits (forward output)\n",
    "pred_out = RNN_2(x, W, b, NUM_HIDDEN_UNITS, seq_max_len, seqLen)\n",
    "\n",
    "######## prepare model ########\n",
    "\n",
    "# loss function = mean-squared error\n",
    "cost = tf.reduce_mean(tf.square(pred_out - y))\n",
    "\n",
    "# optimizer = Adam\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE, name='Adam-op').minimize(cost)\n",
    " \n",
    "# initialize tf variables\n",
    "init = tf.global_variables_initializer()\n",
    " \n",
    "######## train model ########\n",
    "\n",
    "# with: create a tf session, and initialize session variables\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    print('----------Training---------')\n",
    "    for i in range(EPOCHS):\n",
    "        x_batch, y_batch, seq_len_batch = next_batch_2(x_train, y_train, seq_len_train, BATCH_SIZE)\n",
    "        _, mse = sess.run([optimizer, cost], feed_dict={x: x_batch, y: y_batch, seqLen: seq_len_batch})\n",
    "        \n",
    "        if i % DISPLAY_FREQ == 0:\n",
    "            print('Step {0:<6}, MSE={1:.4f}'.format(i, mse))\n",
    "    \n",
    "    ######## test model ########\n",
    "    y_pred = sess.run(pred_out, feed_dict={x: x_test, seqLen: seq_len_test})\n",
    "    \n",
    "    print()\n",
    "    print('--------Test Results-------')\n",
    "    for i, x in enumerate(y_test):\n",
    "        print(\"When the ground truth output is {}, the model thinks it is {}\"\n",
    "              .format(y_test[i], y_pred[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Functions Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(x, y, batch_size):\n",
    "    N = x.shape[0]\n",
    "    batch_indices = np.random.permutation(N)[:batch_size]\n",
    "    x_batch = x[batch_indices]\n",
    "    y_batch = y[batch_indices]\n",
    "    return x_batch, y_batch\n",
    "\n",
    "# weight and bais wrappers\n",
    "def weight_variable(shape):\n",
    "    \"\"\"\n",
    "    Create a weight variable with appropriate initialization\n",
    "    :param name: weight name\n",
    "    :param shape: weight shape\n",
    "    :return: initialized weight variable\n",
    "    \"\"\"\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('W',\n",
    "                           dtype=tf.float32,\n",
    "                           shape=shape,\n",
    "                           initializer=initer)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"\n",
    "    Create a bias variable with appropriate initialization\n",
    "    :param name: bias variable name\n",
    "    :param shape: bias variable shape\n",
    "    :return: initialized bias variable\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b',\n",
    "                           dtype=tf.float32,\n",
    "                           initializer=initial)\n",
    "def RNN(x, weights, biases, num_hidden):\n",
    "    \"\"\"\n",
    "    :param x: inputs of size [batch_size, max_time, input_dim]\n",
    "    :param weights: matrix of fully-connected output layer weights\n",
    "    :param biases: vector of fully-connected output layer biases\n",
    "    :param num_hidden: number of hidden units\n",
    "    \"\"\"\n",
    "    cell = tf.nn.rnn_cell.BasicRNNCell(num_hidden)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "    out = tf.matmul(outputs[:, -1, :], weights) + biases\n",
    "    return out\n",
    "\n",
    "def RNN_2(x, weights, biases, n_hidden, seq_max_len, seq_len):\n",
    "    \"\"\"\n",
    "    :param x: inputs of shape [batch_size, max_time, input_dim]\n",
    "    :param weights: matrix of fully-connected output layer weights\n",
    "    :param biases: vector of fully-connected output layer biases\n",
    "    :param n_hidden: number of hidden units\n",
    "    :param seq_max_len: sequence maximum length\n",
    "    :param seq_len: length of each sequence of shape [batch_size,]\n",
    "    \"\"\"\n",
    "    cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, x, sequence_length=seq_len, dtype=tf.float32)\n",
    "\n",
    "    # Hack to build the indexing and retrieve the right output.\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    # Start indices for each sample\n",
    "    index = tf.range(0, batch_size) * seq_max_len + (seq_len - 1)\n",
    "    # Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n",
    "    out = tf.matmul(outputs, weights) + biases\n",
    "    return out\n",
    "\n",
    "def next_batch_2(x, y, seq_len, batch_size):\n",
    "    N = x.shape[0]\n",
    "    batch_indeces = np.random.permutation(N)[:batch_size]\n",
    "    x_batch = x[batch_indeces]\n",
    "    y_batch = y[batch_indeces]\n",
    "    seq_len_batch = seq_len[batch_indeces]\n",
    "    return x_batch, y_batch, seq_len_batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
