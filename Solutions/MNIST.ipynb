{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST digits classification: 4 solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source (all credits to):** *Coursera: Intro to Deep Learning Course* and *Easy TensorFlow*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../resources/mnist_sample.png\" style=\"width:30%\">\n",
    "\n",
    "Solutions:\n",
    "\n",
    "1. Logistic Regression.\n",
    "2. Multilayer Perceptron (MLP) with 2 Hidden Layers.\n",
    "3. Convolutional Neural Network (CNN).\n",
    "4. Recurrent Neural Network (RNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're using TensorFlow 2.4.1\n",
      "We're using Keras 2.4.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf # pip install tensorflow 1.2.1\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib_utils\n",
    "from importlib import reload\n",
    "reload(matplotlib_utils)\n",
    "\n",
    "print(\"We're using TensorFlow\", tf.__version__)\n",
    "print(\"We're using Keras\", keras.__version__)\n",
    "\n",
    "# use TensorFlow v.1\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_dataset(flatten=False):\n",
    "    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # normalize x\n",
    "    X_train = X_train.astype(float) / 255.\n",
    "    X_test = X_test.astype(float) / 255.\n",
    "\n",
    "    # we reserve the last 10000 training examples for validation\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    if flatten:\n",
    "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
    "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
    "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "---\n",
      "X_train = (50000, 28, 28)\n",
      "y_train = (50000,)\n",
      "X_test = (10000, 28, 28)\n",
      "y_test = (10000,)\n",
      "\n",
      "Shapes for linear model\n",
      "---\n",
      "Flat X_train = (50000, 784)\n",
      "Flat X_val = (10000, 784)\n",
      "\n",
      "Shapes for cross-entropy\n",
      "---\n",
      "One-hot-encoded y_train = (50000, 10)\n",
      "One-hot-encoded y_val = (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = preprocessing_dataset()\n",
    "\n",
    "print('Shapes')\n",
    "print('---')\n",
    "print('X_train =', X_train.shape)\n",
    "print('y_train =', y_train.shape)\n",
    "print('X_test =', X_test.shape)\n",
    "print('y_test =', y_test.shape)\n",
    "print()\n",
    "print('Shapes for linear model')\n",
    "print('---')\n",
    "\n",
    "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
    "print('Flat X_train =', X_train_flat.shape)\n",
    "\n",
    "X_val_flat = X_val.reshape((X_val.shape[0], -1))\n",
    "print('Flat X_val =', X_val_flat.shape)\n",
    "print()\n",
    "print('Shapes for cross-entropy')\n",
    "print('---')\n",
    "\n",
    "y_train_oh = keras.utils.to_categorical(y_train, 10)\n",
    "y_val_oh = keras.utils.to_categorical(y_val, 10)\n",
    "\n",
    "print('One-hot-encoded y_train =', y_train_oh.shape)\n",
    "print('One-hot-encoded y_val =', y_val_oh.shape)\n",
    "\n",
    "X_test_flat = X_test.reshape((X_test.shape[0], -1))\n",
    "y_test_oh = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Look at Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a manuscript digit\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample patch of the manuscript digit\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Example of a manuscript digit')\n",
    "\n",
    "plt.imshow(X_train[2], cmap=\"Greys\")\n",
    "plt.show()\n",
    "\n",
    "print('Sample patch of the manuscript digit')\n",
    "\n",
    "plt.imshow(X_train[2, 10:20, 5:15], cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Solution 1 - Linear Model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../resources/linear_classifier.png\" style=\"width:80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dimensions\n",
    "input_size = len(X_train_flat[1])\n",
    "n_classes = len(y_train_oh[1])\n",
    "\n",
    "# clear context\n",
    "K.clear_session()\n",
    "\n",
    "# hyper-parameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 100\n",
    "DISPLAY_FREQ = 100      # frequency of displaying the training results\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# model input data\n",
    "x = tf.placeholder(tf.float32, shape=[None, input_size], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_classes], name='Y')\n",
    " \n",
    "# model parameters: W (weights) and b (bias)\n",
    "W = weight_variable(shape=[input_size, n_classes])\n",
    "b = bias_variable(shape=[n_classes])\n",
    "\n",
    "# calculate logits (forward output)\n",
    "#output_logits = tf.matmul(x, W) + b\n",
    "output_logits = x @ W + b\n",
    "\n",
    "######## prepare model ########\n",
    "\n",
    "# loss function = softmax with cross-entropy\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\n",
    "\n",
    "# optimizer = Adam\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE, name='Adam-op').minimize(loss)\n",
    "\n",
    "# model prediction\n",
    "correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')\n",
    "\n",
    "# model accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "\n",
    "# model predictions\n",
    "cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')\n",
    " \n",
    "# initialize tf variables\n",
    "init = tf.global_variables_initializer()\n",
    " \n",
    "######## train model ########\n",
    "\n",
    "# create a tf session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# initialize session variables\n",
    "sess.run(init)\n",
    "\n",
    "num_tr_iter = int(len(y_train) / BATCH_SIZE) # ~500 iterations\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('Training epoch: {}'.format(epoch + 1))\n",
    "    \n",
    "    # randomly shuffle the training data at the beginning of each epoch \n",
    "    x_train, y_train = randomize(X_train_flat, y_train_oh)\n",
    "    \n",
    "    for iteration in range(num_tr_iter):\n",
    "        start = iteration * BATCH_SIZE\n",
    "        end = (iteration + 1) * BATCH_SIZE\n",
    "        \n",
    "        x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\n",
    "\n",
    "        # run optimization (backprop)\n",
    "        feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "        sess.run(optimizer, feed_dict=feed_dict_batch)\n",
    "\n",
    "        if iteration % DISPLAY_FREQ == 0:\n",
    "            \n",
    "            # calculate and display the batch loss and accuracy\n",
    "            loss_batch, acc_batch = sess.run([loss, accuracy], feed_dict=feed_dict_batch)\n",
    "\n",
    "            print(\"iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}\".\n",
    "                  format(iteration, loss_batch, acc_batch))\n",
    "\n",
    "    # run validation after every epoch\n",
    "    feed_dict_valid = {x: X_val_flat[:1000], y: y_val_oh[:1000]}\n",
    "    loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
    "    \n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
    "          format(epoch + 1, loss_valid, acc_valid))\n",
    "    print('---------------------------------------------------------')\n",
    "    \n",
    "######## test model ########\n",
    "\n",
    "feed_dict_test = {x: X_test_flat, y: y_test_oh}\n",
    "loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_test)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print(\"Test loss: {0:.2f}, test accuracy: {1:.01%}\".format(loss_test, acc_test))\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "######## plot results ########\n",
    "\n",
    "# plot some of the correct and misclassified examples\n",
    "cls_pred = sess.run(cls_prediction, feed_dict=feed_dict_test)\n",
    "cls_true = np.argmax(y_test_oh, axis=1)\n",
    "\n",
    "plot_images(X_test_flat, cls_true, cls_pred, title='Correct Examples')\n",
    "plot_example_errors(X_test_flat, cls_true, cls_pred, title='Misclassified Examples')\n",
    "plt.show()\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Solution 2 - MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../resources/nn.png\" style=\"width:100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dimensions\n",
    "input_size = len(X_train_flat[1])\n",
    "n_classes = len(y_train_oh[1])\n",
    "\n",
    "# clear context\n",
    "K.clear_session()\n",
    "\n",
    "# model input data\n",
    "x = tf.placeholder(tf.float32, shape=[None, input_size], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_classes], name='Y')\n",
    " \n",
    "# model parameters: W (weights) and b (bias)\n",
    "W = weight_variable(shape=[input_size, n_classes])\n",
    "b = bias_variable(shape=[n_classes])\n",
    "\n",
    "# hyper-parameters\n",
    "EPOCHS = 13\n",
    "BATCH_SIZE = 100\n",
    "DISPLAY_FREQ = 100      # frequency of displaying the training results\n",
    "LEARNING_RATE = 0.001\n",
    "h1 = 200                # number of neurons in the 1st hidden layey\n",
    "\n",
    "######## prepare model ########\n",
    "\n",
    "# create a fully-connected layer with h1 nodes as hidden layer\n",
    "fc1 = fc_layer(x, h1, 'FC1', use_relu=True)\n",
    "\n",
    "# create a fully-connected layer with n_classes nodes as output layer\n",
    "output_logits = fc_layer(fc1, n_classes, 'OUT', use_relu=False)\n",
    "\n",
    "# loss function = softmax with cross-entropy\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\n",
    "\n",
    "# optimizer = Adam\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE, name='Adam-op').minimize(loss)\n",
    "\n",
    "# model prediction\n",
    "correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')\n",
    "\n",
    "# model accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "\n",
    "# model predictions\n",
    "cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')\n",
    " \n",
    "# initialize tf variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# model predictions\n",
    "cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')\n",
    " \n",
    "# initialize tf variables\n",
    "init = tf.global_variables_initializer()\n",
    " \n",
    "######## train model ########\n",
    "\n",
    "# create a tf session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# initialize session variables\n",
    "sess.run(init)\n",
    "\n",
    "num_tr_iter = int(len(y_train) / BATCH_SIZE) # ~500 iterations\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('Training epoch: {}'.format(epoch + 1))\n",
    "    \n",
    "    # randomly shuffle the training data at the beginning of each epoch \n",
    "    x_train, y_train = randomize(X_train_flat, y_train_oh)\n",
    "    \n",
    "    for iteration in range(num_tr_iter):\n",
    "        start = iteration * BATCH_SIZE\n",
    "        end = (iteration + 1) * BATCH_SIZE\n",
    "        \n",
    "        x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\n",
    "\n",
    "        # run optimization (backprop)\n",
    "        feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "        sess.run(optimizer, feed_dict=feed_dict_batch)\n",
    "\n",
    "        if iteration % DISPLAY_FREQ == 0:\n",
    "            \n",
    "            # calculate and display the batch loss and accuracy\n",
    "            loss_batch, acc_batch = sess.run([loss, accuracy], feed_dict=feed_dict_batch)\n",
    "\n",
    "            print(\"iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}\".\n",
    "                  format(iteration, loss_batch, acc_batch))\n",
    "\n",
    "    # run validation after every epoch\n",
    "    feed_dict_valid = {x: X_val_flat[:1000], y: y_val_oh[:1000]}\n",
    "    loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
    "    \n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
    "          format(epoch + 1, loss_valid, acc_valid))\n",
    "    print('---------------------------------------------------------')\n",
    "    \n",
    "######## test model ########\n",
    "feed_dict_test = {x: X_test_flat, y: y_test_oh}\n",
    "loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_test)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print(\"Test loss: {0:.2f}, test accuracy: {1:.01%}\".format(loss_test, acc_test))\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "######## plot results ########\n",
    "\n",
    "# plot some of the correct and misclassified examples\n",
    "cls_pred = sess.run(cls_prediction, feed_dict=feed_dict_test)\n",
    "cls_true = np.argmax(y_test_oh, axis=1)\n",
    "\n",
    "plot_images(X_test_flat, cls_true, cls_pred, title='Correct Examples')\n",
    "plot_example_errors(X_test_flat, cls_true, cls_pred, title='Misclassified Examples')\n",
    "plt.show()\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Solution 3 - CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../resources/cnn.png\" style=\"width:100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dimensions\n",
    "input_size = len(X_train_flat[1])\n",
    "n_classes = len(y_train_oh[1])\n",
    "img_h = len(X_train[1])         # MNIST images are 28x28\n",
    "img_w = len(X_train[0])\n",
    "img_size_flat = img_h * img_w   # 28x28=784, the total number of pixels\n",
    "n_channels = 1\n",
    "\n",
    "# clear context\n",
    "K.clear_session()\n",
    "\n",
    "# model input data\n",
    "\n",
    "# reformats the data to the format acceptable for convolutional layers\n",
    "x_train, _ = reformat(X_train_flat, y_train_oh)\n",
    "x_valid, _ = reformat(X_val_flat, y_val_oh)\n",
    "x_test, _ = reformat(X_test_flat, y_test_oh)\n",
    "\n",
    "with tf.name_scope('Input'):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, img_h, img_w, n_channels], name='X')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, n_classes], name='Y')\n",
    "    \n",
    "# hyper-parameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 100\n",
    "DISPLAY_FREQ = 100      # frequency of displaying the training results\n",
    "LEARNING_RATE = 0.001\n",
    "LOGS_PATH = \"./logs\"    # path to the folder that we want to save the logs for Tensorboard\n",
    "\n",
    "######## prepare model ########\n",
    "\n",
    "# network configuration\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "filter_size1 = 5   # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 16  # There are 16 of these filters.\n",
    "stride1 = 1        # The stride of the sliding window\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "filter_size2 = 5   # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 32  # There are 32 of these filters.\n",
    "stride2 = 1        # The stride of the sliding window\n",
    "\n",
    "# Fully-connected layer.\n",
    "h1 = 128           # Number of neurons in fully-connected layer.\n",
    "\n",
    "# create the network layers\n",
    "conv1 = conv_layer(x, filter_size1, num_filters1, stride1, name='conv1')\n",
    "pool1 = max_pool(conv1, ksize=2, stride=2, name='pool1')\n",
    "conv2 = conv_layer(pool1, filter_size2, num_filters2, stride2, name='conv2')\n",
    "pool2 = max_pool(conv2, ksize=2, stride=2, name='pool2')\n",
    "layer_flat = flatten_layer(pool2)\n",
    "fc1 = fc_layer(layer_flat, h1, 'FC1', use_relu=True)\n",
    "output_logits = fc_layer(fc1, n_classes, 'OUT', use_relu=False)\n",
    "\n",
    "# loss function = softmax with cross-entropy\n",
    "with tf.variable_scope('Train'):\n",
    "    with tf.variable_scope('Loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # optimizer = Adam\n",
    "    with tf.variable_scope('Optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE, name='Adam-op').minimize(loss)\n",
    "        \n",
    "    # model prediction\n",
    "    with tf.variable_scope('Accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')\n",
    "\n",
    "    # model accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # model predictions\n",
    "    with tf.variable_scope('Prediction'):\n",
    "        cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')\n",
    "        \n",
    "# initialize tf variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Merge all summaries\n",
    "merged = tf.summary.merge_all()\n",
    " \n",
    "######## train model ########\n",
    "\n",
    "# create a tf session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# initialize session variables\n",
    "sess.run(init)\n",
    "\n",
    "num_tr_iter = int(len(y_train) / BATCH_SIZE) # ~500 iterations\n",
    "\n",
    "# log training proccess\n",
    "global_step = 0\n",
    "summary_writer = tf.summary.FileWriter(LOGS_PATH, sess.graph)\n",
    "\n",
    "# shuffle the before training\n",
    "x_train, y_train = randomize(x_train, y_train_oh)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('Training epoch: {}'.format(epoch + 1))\n",
    "    \n",
    "    # randomly shuffle the training data at the beginning of each epoch \n",
    "    x_train, y_train = randomize(x_train, y_train)\n",
    "    \n",
    "    for iteration in range(num_tr_iter):\n",
    "        global_step += 1\n",
    "        start = iteration * BATCH_SIZE\n",
    "        end = (iteration + 1) * BATCH_SIZE\n",
    "        x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\n",
    "\n",
    "        # run optimization (backprop)\n",
    "        feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "        sess.run(optimizer, feed_dict=feed_dict_batch)\n",
    "\n",
    "        if iteration % DISPLAY_FREQ == 0:\n",
    "            # calculate and display the batch loss and accuracy\n",
    "            loss_batch, acc_batch, summary_tr = sess.run([loss, accuracy, merged],\n",
    "                                                         feed_dict=feed_dict_batch)\n",
    "            summary_writer.add_summary(summary_tr, global_step)\n",
    "\n",
    "            print(\"iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}\".\n",
    "                  format(iteration, loss_batch, acc_batch))\n",
    "\n",
    "    # run validation after every epoch\n",
    "    feed_dict_valid = {x: x_valid, y: y_val_oh}\n",
    "    loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
    "    \n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
    "          format(epoch + 1, loss_valid, acc_valid))\n",
    "    print('---------------------------------------------------------')\n",
    "    \n",
    "######## test model ########\n",
    "feed_dict_test = {x: x_test, y: y_test_oh}\n",
    "loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_test)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print(\"Test loss: {0:.2f}, test accuracy: {1:.01%}\".format(loss_test, acc_test))\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "######## plot results ########\n",
    "\n",
    "# plot some of the correct and misclassified examples\n",
    "cls_pred = sess.run(cls_prediction, feed_dict=feed_dict_test)\n",
    "cls_true = np.argmax(y_test_oh, axis=1)\n",
    "\n",
    "plot_images(X_test_flat, cls_true, cls_pred, title='Correct Examples')\n",
    "plot_example_errors(x_test, cls_true, cls_pred, title='Misclassified Examples')\n",
    "plt.show()\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Solution 4 - RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../resources/rnn.png\" style=\"width:80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing...\n"
     ]
    }
   ],
   "source": [
    "print('Doing...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 99. Functions Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    \"\"\"\n",
    "    Create a weight variable with appropriate initialization\n",
    "    :param name: weight name\n",
    "    :param shape: weight shape\n",
    "    :return: initialized weight variable\n",
    "    \"\"\"\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('W',\n",
    "                           dtype=tf.float32,\n",
    "                           shape=shape,\n",
    "                           initializer=initer)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"\n",
    "    Create a bias variable with appropriate initialization\n",
    "    :param name: bias variable name\n",
    "    :param shape: bias variable shape\n",
    "    :return: initialized bias variable\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b',\n",
    "                           dtype=tf.float32,\n",
    "                           initializer=initial)\n",
    "\n",
    "def randomize(x, y):\n",
    "    \"\"\" Randomizes the order of data samples and their corresponding labels\"\"\"\n",
    "    permutation = np.random.permutation(y.shape[0])\n",
    "    shuffled_x = x[permutation, :]\n",
    "    shuffled_y = y[permutation]\n",
    "    return shuffled_x, shuffled_y\n",
    "\n",
    "def get_next_batch(x, y, start, end):\n",
    "    x_batch = x[start:end]\n",
    "    y_batch = y[start:end]\n",
    "    return x_batch, y_batch\n",
    "\n",
    "def plot_images(images, cls_true, cls_pred=None, title=None):\n",
    "    \"\"\"\n",
    "    Create figure with 3x3 sub-plots.\n",
    "    :param images: array of images to be plotted, (9, img_h*img_w)\n",
    "    :param cls_true: corresponding true labels (9,)\n",
    "    :param cls_pred: corresponding true labels (9,)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(28, 28), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            ax_title = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            ax_title = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_title(ax_title)\n",
    "\n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    if title:\n",
    "        plt.suptitle(title, size=20)\n",
    "    plt.show(block=False)\n",
    "\n",
    "def plot_example_errors(images, cls_true, cls_pred, title=None):\n",
    "    \"\"\"\n",
    "    Function for plotting examples of images that have been mis-classified\n",
    "    :param images: array of all images, (#imgs, img_h*img_w)\n",
    "    :param cls_true: corresponding true labels, (#imgs,)\n",
    "    :param cls_pred: corresponding predicted labels, (#imgs,)\n",
    "    \"\"\"\n",
    "    # Negate the boolean array.\n",
    "    incorrect = np.logical_not(np.equal(cls_pred, cls_true))\n",
    "\n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    incorrect_images = images[incorrect]\n",
    "\n",
    "    # Get the true and predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "    cls_true = cls_true[incorrect]\n",
    "\n",
    "    # Plot the first 9 images.\n",
    "    plot_images(images=incorrect_images[0:9],\n",
    "                cls_true=cls_true[0:9],\n",
    "                cls_pred=cls_pred[0:9],\n",
    "                title=title)\n",
    "    \n",
    "# weight wrapper for Neural Network (NN)\n",
    "def weight_variable_NN(name, shape):\n",
    "    \"\"\"\n",
    "    Create a weight variable with appropriate initialization\n",
    "    :param name: weight name\n",
    "    :param shape: weight shape\n",
    "    :return: initialized weight variable\n",
    "    \"\"\"\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('W_' + name,\n",
    "                           dtype=tf.float32,\n",
    "                           shape=shape,\n",
    "                           initializer=initer)\n",
    "\n",
    "# bias wrapper for Neural Network (NN)\n",
    "def bias_variable_NN(name, shape):\n",
    "    \"\"\"\n",
    "    Create a bias variable with appropriate initialization\n",
    "    :param name: bias variable name\n",
    "    :param shape: bias variable shape\n",
    "    :return: initialized bias variable\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b_' + name,\n",
    "                           dtype=tf.float32,\n",
    "                           initializer=initial)\n",
    "\n",
    "def fc_layer(x, num_units, name, use_relu=True):\n",
    "    \"\"\"\n",
    "    Create a fully-connected layer\n",
    "    :param x: input from previous layer\n",
    "    :param num_units: number of hidden units in the fully-connected layer\n",
    "    :param name: layer name\n",
    "    :param use_relu: boolean to add ReLU non-linearity (or not)\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    in_dim = x.get_shape()[1]\n",
    "    W = weight_variable_NN(name, shape=[in_dim, num_units])\n",
    "    b = bias_variable_NN(name, [num_units])\n",
    "    layer = tf.matmul(x, W)\n",
    "    layer += b\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    return layer\n",
    "\n",
    "def reformat(x, y):\n",
    "    \"\"\"\n",
    "    Reformats the data to the format acceptable for convolutional layers\n",
    "    :param x: input array\n",
    "    :param y: corresponding labels\n",
    "    :return: reshaped input and labels\n",
    "    \"\"\"\n",
    "    img_size, num_ch, num_class = int(np.sqrt(x.shape[-1])), 1, len(np.unique(np.argmax(y, 1)))\n",
    "    dataset = x.reshape((-1, img_size, img_size, num_ch)).astype(np.float32)\n",
    "    labels = (np.arange(num_class) == y[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "def conv_layer(x, filter_size, num_filters, stride, name):\n",
    "    \"\"\"\n",
    "    Create a 2D convolution layer\n",
    "    :param x: input from previous layer\n",
    "    :param filter_size: size of each filter\n",
    "    :param num_filters: number of filters (or output feature maps)\n",
    "    :param stride: filter stride\n",
    "    :param name: layer name\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        num_in_channel = x.get_shape().as_list()[-1]\n",
    "        shape = [filter_size, filter_size, num_in_channel, num_filters]\n",
    "        W = weight_variable(shape=shape)\n",
    "        tf.summary.histogram('weight', W)\n",
    "        b = bias_variable(shape=[num_filters])\n",
    "        tf.summary.histogram('bias', b)\n",
    "        layer = tf.nn.conv2d(x, W,\n",
    "                             strides=[1, stride, stride, 1],\n",
    "                             padding=\"SAME\")\n",
    "        layer += b\n",
    "        return tf.nn.relu(layer)\n",
    "\n",
    "\n",
    "def max_pool(x, ksize, stride, name):\n",
    "    \"\"\"\n",
    "    Create a max pooling layer\n",
    "    :param x: input to max-pooling layer\n",
    "    :param ksize: size of the max-pooling filter\n",
    "    :param stride: stride of the max-pooling filter\n",
    "    :param name: layer name\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(x,\n",
    "                          ksize=[1, ksize, ksize, 1],\n",
    "                          strides=[1, stride, stride, 1],\n",
    "                          padding=\"SAME\",\n",
    "                          name=name)\n",
    "\n",
    "def flatten_layer(layer):\n",
    "    \"\"\"\n",
    "    Flattens the output of the convolutional layer to be fed into fully-connected layer\n",
    "    :param layer: input array\n",
    "    :return: flattened array\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Flatten_layer'):\n",
    "        layer_shape = layer.get_shape()\n",
    "        num_features = layer_shape[1:4].num_elements()\n",
    "        layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    return layer_flat\n",
    "\n",
    "def fc_layer(x, num_units, name, use_relu=True):\n",
    "    \"\"\"\n",
    "    Create a fully-connected layer\n",
    "    :param x: input from previous layer\n",
    "    :param num_units: number of hidden units in the fully-connected layer\n",
    "    :param name: layer name\n",
    "    :param use_relu: boolean to add ReLU non-linearity (or not)\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        in_dim = x.get_shape()[1]\n",
    "        W = weight_variable(shape=[in_dim, num_units])\n",
    "        tf.summary.histogram('weight', W)\n",
    "        b = bias_variable(shape=[num_units])\n",
    "        tf.summary.histogram('bias', b)\n",
    "        layer = tf.matmul(x, W)\n",
    "        layer += b\n",
    "        if use_relu:\n",
    "            layer = tf.nn.relu(layer)\n",
    "        return layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
